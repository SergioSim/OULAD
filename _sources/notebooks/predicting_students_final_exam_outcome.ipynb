{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c09d0bbd",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Predicting students final exam outcome\n",
    "\n",
    "This section aims to predict the `student final exam outcome`\n",
    "(Pass (score >= 40) / Fail (score < 40)).\n",
    "We try to replicate the machine learning analysis techinques from the work of\n",
    "Tomasevic et al. (2020) {cite}`tomasevic_2020`.\n",
    "\n",
    "**Keywords**: Predicting student outcome\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdf67e2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from multicons import MultiCons\n",
    "\n",
    "from oulad import get_oulad\n",
    "\n",
    "oulad = get_oulad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6bad23",
   "metadata": {
    "lines_to_next_cell": 2,
    "user_expressions": []
   },
   "source": [
    "## Preparing train/test data\n",
    "\n",
    "### Selecting features\n",
    "\n",
    "In the work of Tomasevic et al. the student data from the `DDD` module of the\n",
    "`2013J` and `2014B` presentations combined is used.\n",
    "\n",
    "Similarly, we try to select the same seven distinct attributes from the three distinct\n",
    "types below:\n",
    "\n",
    "<table class=\"colwidths-auto table\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th class=\"text-center head\"><p>DEMOGRAPHIC</p></th>\n",
    "            <th class=\"text-center head\"><p>ENGAGEMENT</p></th>\n",
    "            <th class=\"text-center head\"><p>PERFORMANCE</p></th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td class=\"text-left\">\n",
    "                <ul>\n",
    "                    <li>gender</li>\n",
    "                    <li>highest_education</li>\n",
    "                    <li>age_band</li>\n",
    "                </ul>\n",
    "            </td>\n",
    "            <td class=\"text-left\">\n",
    "                <ul>\n",
    "                    <li>sum of clicks per assessment</li>\n",
    "                </ul>\n",
    "            </td>\n",
    "            <td class=\"text-left\">\n",
    "                <ul>\n",
    "                    <li>scores per assessment</li>\n",
    "                    <li>number of attempts</li>\n",
    "                    <li>final_exam score</li>\n",
    "                </ul>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e277708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_table(max_date=500, code_presentation=\"2013J\"):\n",
    "    \"\"\"Returns the feature table computed from the OULAD dataset.\"\"\"\n",
    "\n",
    "    # Select all assessments from the module.\n",
    "    assessments = oulad.assessments[\n",
    "        (oulad.assessments.code_module == \"DDD\")\n",
    "        & (oulad.assessments.code_presentation == code_presentation)\n",
    "    ]\n",
    "\n",
    "    # Filter out assessments that are after the max_date\n",
    "    assessments = assessments[\n",
    "        (assessments.date <= max_date) | (assessments.assessment_type == \"Exam\")\n",
    "    ]\n",
    "\n",
    "    # Filter relevant rows and columns from the student_vle table.\n",
    "    vle = oulad.student_vle.loc[\n",
    "        (oulad.student_vle.code_module == \"DDD\")\n",
    "        & (oulad.student_vle.code_presentation == code_presentation),\n",
    "        [\"id_student\", \"date\", \"sum_click\"],\n",
    "    ]\n",
    "\n",
    "    # Categorize the date field by assessment date.\n",
    "    previous_date = None\n",
    "    for date in assessments.date:\n",
    "        if previous_date:\n",
    "            vle.loc[(vle.date > previous_date) & (vle.date < date), \"date\"] = date\n",
    "        else:\n",
    "            vle.loc[vle.date < date, \"date\"] = date\n",
    "        previous_date = date\n",
    "\n",
    "    # Sum scores by date.\n",
    "    vle = vle.groupby([\"id_student\", \"date\"]).agg(np.sum).reset_index()\n",
    "\n",
    "    # Reshape the vle table.\n",
    "    vle = vle.pivot(index=\"id_student\", columns=\"date\", values=\"sum_click\")\n",
    "\n",
    "    # Rename columns\n",
    "    vle = vle.rename(\n",
    "        columns={\n",
    "            assessment.date: f\"assessment_{i+1}_sum_click\"\n",
    "            if assessment.assessment_type != \"Exam\"\n",
    "            else \"final_exam_sum_click\"\n",
    "            for i, (_, assessment) in enumerate(assessments.iterrows())\n",
    "        }\n",
    "    ).drop(\"final_exam_sum_click\", axis=1)\n",
    "\n",
    "    return (\n",
    "        oulad.student_info.loc[\n",
    "            (oulad.student_info.code_module == \"DDD\")\n",
    "            & (oulad.student_info.code_presentation == code_presentation),\n",
    "            [\n",
    "                \"id_student\",\n",
    "                \"gender\",\n",
    "                \"highest_education\",\n",
    "                \"age_band\",\n",
    "                \"num_of_prev_attempts\",\n",
    "                # The `final_result` column is only used to fill missing `final_exam`\n",
    "                # values, it should be removed from the training set.\n",
    "                \"final_result\",\n",
    "            ],\n",
    "        ]\n",
    "        .set_index(\"id_student\")\n",
    "        .join(vle)\n",
    "        .join(\n",
    "            oulad.student_assessment[\n",
    "                oulad.student_assessment.id_assessment.isin(assessments.id_assessment)\n",
    "            ]\n",
    "            .pivot(index=\"id_student\", columns=\"id_assessment\", values=\"score\")\n",
    "            .rename(\n",
    "                columns={\n",
    "                    assessment.id_assessment: f\"assessment_{i+1}_score\"\n",
    "                    if assessment.assessment_type != \"Exam\"\n",
    "                    else \"final_exam_score\"\n",
    "                    for i, (_, assessment) in enumerate(assessments.iterrows())\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "feature_table = pd.concat(\n",
    "    [get_feature_table(), get_feature_table(code_presentation=\"2014B\")]\n",
    ")\n",
    "display(feature_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813a79e9",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Pre-Processing\n",
    "\n",
    "#### Handling NAs\n",
    "\n",
    "We notice many missing values from the `final_exam_score` column in the selected\n",
    "feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d27adf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"The feature table has {len(feature_table)} rows and the final exam score \"\n",
    "    f\"column has {feature_table.final_exam_score.isna().sum()} rows with NAs \"\n",
    "    f\"({100*feature_table.final_exam_score.isna().sum() / len(feature_table):.0f}%).\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c19a66",
   "metadata": {
    "lines_to_next_cell": 2,
    "user_expressions": []
   },
   "source": [
    "This is explained in the original OULAD paper of Kuzilek et al.\n",
    "[\\[KHZ17\\]](../notebooks/first_descriptive_analysis.html#id1):\n",
    "```\n",
    "Results of the final exam are usually missing (since they are scored and used for the\n",
    "final marking immediately at the end of the module).\n",
    "```\n",
    "\n",
    "Therefore we use the `final_results` column to fill the missing final exam\n",
    "values and then remove the `final_results` column.\n",
    "\n",
    "Other columns containing missing values we fill with the value `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb0a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nas(feature_table_df):\n",
    "    \"\"\"Fills NAs in the `final_exam_score` column with `final_result` values,\n",
    "    drops the `final_result` column and fills remaining NAs with the value `-1`.\n",
    "    \"\"\"\n",
    "\n",
    "    final_exam_score_nas = feature_table.final_exam_score.isna()\n",
    "    feature_table_df.loc[final_exam_score_nas, \"final_exam_score\"] = (\n",
    "        feature_table[final_exam_score_nas].final_result.isin([\"Pass\", \"Distinction\"])\n",
    "        * 40\n",
    "    )\n",
    "    return feature_table.drop(columns=\"final_result\").fillna(-1)\n",
    "\n",
    "\n",
    "feature_table = fill_nas(feature_table)\n",
    "display(feature_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6dc7728",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### Splitting train / test data and Normalization\n",
    "\n",
    "Now we randomly split the feature table rows into a train (80%) and test (20%) table\n",
    "and, as in the work of Tomasevic et al., we scale and nomalize the selected features:\n",
    "\n",
    "<table class=\"colwidths-auto table\">\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th class=\"text-center head\"><p>Feature</p></th>\n",
    "            <th class=\"text-center head\"><p>Normalization</p></th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td class=\"text-left\">\n",
    "                <ul>\n",
    "                    <li>gender</li>\n",
    "                </ul>\n",
    "            </td>\n",
    "            <td class=\"text-left\">\n",
    "                <ul>\n",
    "                    <li>0 = male</li>\n",
    "                    <li>1 = female</li>\n",
    "                </ul>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td class=\"text-left\">\n",
    "                <ul>\n",
    "                    <li>age_band</li>\n",
    "                </ul>\n",
    "            </td>\n",
    "            <td class=\"text-left\">\n",
    "                <ul>\n",
    "                    <li>0.0 = 0-35</li>\n",
    "                    <li>0.5 = 35-55</li>\n",
    "                    <li>1.0 = 55&lt;=</li>\n",
    "                </ul>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td class=\"text-left\">\n",
    "                <ul>\n",
    "                    <li>highest_education</li>\n",
    "                </ul>\n",
    "            </td>\n",
    "            <td class=\"text-left\">\n",
    "                <ul>\n",
    "                    <li>0.00 = No Formal quals</li>\n",
    "                    <li>0.25 = Lower Than A Level</li>\n",
    "                    <li>0.50 = A Level or Equivalent</li>\n",
    "                    <li>0.75 = HE Qualification</li>\n",
    "                    <li>1.00 = Post Graduate Qualification</li>\n",
    "                </ul>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td class=\"text-left\">\n",
    "                <ul>\n",
    "                    <li>number of attempts</li>\n",
    "                    <li>sum of clicks per assessment</li>\n",
    "                </ul>\n",
    "            </td>\n",
    "            <td class=\"text-left\">\n",
    "                <ul>\n",
    "                    <li>0-N scaled to [0-1]</li>\n",
    "                </ul>\n",
    "            </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td class=\"text-left\">\n",
    "                <ul>\n",
    "                    <li>scores per assessment</li>\n",
    "                    <li>final_exam_score</li>\n",
    "                </ul>\n",
    "            </td>\n",
    "            <td class=\"text-left\">\n",
    "                <ul>\n",
    "                    <li>0-100 scaled to [0-1]</li>\n",
    "                </ul>\n",
    "            </td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d0fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 0\n",
    "\n",
    "\n",
    "def normalized_train_test_split(feature_table_df, is_for_classification=True):\n",
    "    \"\"\"Returns the normalized tain/test split computed form the feature table.\n",
    "\n",
    "    If `is_for_classification` is set to true (default) the final_exam_score will be\n",
    "    converted into two classes 0 (score < 40 == Fail) and 1 (score >= 40 == Pass).\n",
    "    \"\"\"\n",
    "\n",
    "    x_train_, x_test_, y_train_, y_test_ = train_test_split(\n",
    "        feature_table_df.drop(columns=\"final_exam_score\"),\n",
    "        feature_table_df[\"final_exam_score\"],\n",
    "        test_size=0.2,\n",
    "        random_state=RANDOM_STATE,\n",
    "    )\n",
    "\n",
    "    # Scale scores per assessment and final_exam_score.\n",
    "    assessment_score_labels = feature_table_df.columns.values[\n",
    "        feature_table_df.columns.str.match(r\"assessment_[0-9]+_score\")\n",
    "    ]\n",
    "    x_train_.loc[:, assessment_score_labels] /= 100\n",
    "    x_test_.loc[:, assessment_score_labels] /= 100\n",
    "    y_train_ /= 100\n",
    "    y_test_ /= 100\n",
    "    if is_for_classification:\n",
    "        y_train_ = (y_train_ >= 0.4).astype(int)\n",
    "        y_test_ = (y_test_ >= 0.4).astype(int)\n",
    "\n",
    "    # Transform gender, age_band and highest_education to numeric values.\n",
    "    gender_map = {\"M\": 0, \"F\": 1}\n",
    "    age_band_map = {\"0-35\": 0, \"35-55\": 0.5, \"55<=\": 1}\n",
    "    highest_education_map = {\n",
    "        \"No Formal quals\": 0,\n",
    "        \"Lower Than A Level\": 0.25,\n",
    "        \"A Level or Equivalent\": 0.5,\n",
    "        \"HE Qualification\": 0.75,\n",
    "        \"Post Graduate Qualification\": 1,\n",
    "    }\n",
    "\n",
    "    x_train_.loc[:, \"gender\"] = x_train_.loc[:, \"gender\"].map(gender_map)\n",
    "    x_test_.loc[:, \"gender\"] = x_test_.loc[:, \"gender\"].map(gender_map)\n",
    "\n",
    "    x_train_.loc[:, \"age_band\"] = x_train_.loc[:, \"age_band\"].map(age_band_map)\n",
    "    x_test_.loc[:, \"age_band\"] = x_test_.loc[:, \"age_band\"].map(age_band_map)\n",
    "\n",
    "    x_train_.loc[:, \"highest_education\"] = x_train_.loc[:, \"highest_education\"].map(\n",
    "        highest_education_map\n",
    "    )\n",
    "    x_test_.loc[:, \"highest_education\"] = x_test_.loc[:, \"highest_education\"].map(\n",
    "        highest_education_map\n",
    "    )\n",
    "\n",
    "    # Scale sum of click per assessment and number of attempts.\n",
    "    columns_slice = feature_table_df.columns.values[\n",
    "        feature_table_df.columns.str.match(r\"assessment_[0-9]+_sum_click\")\n",
    "    ].tolist() + [\"num_of_prev_attempts\"]\n",
    "\n",
    "    # Note: we fit the scaler only on the train data to avoid \"leaking\" information\n",
    "    # from the test data.\n",
    "    scaler = MinMaxScaler().fit(x_train_.loc[:, columns_slice])\n",
    "    x_train_.loc[:, columns_slice] = scaler.transform(x_train_.loc[:, columns_slice])\n",
    "    x_test_.loc[:, columns_slice] = scaler.transform(x_test_.loc[:, columns_slice])\n",
    "    return (x_train_, x_test_, y_train_, y_test_)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = normalized_train_test_split(feature_table)\n",
    "display(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b4a752",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Classification\n",
    "\n",
    "As in the work of Tomasevic et al., we will compare the classification performances\n",
    "for the student final exam pass prediction (score >= 40).\n",
    "\n",
    "We use the same models and try to perform a grid search over the same Hyper-parameter\n",
    "ranges if these were specified in the paper:\n",
    "\n",
    "- K-Nearest Neighbours (with & without `weights`, varying `K` between 1 and 50)\n",
    "- Support Vector Machines (with `linear` and `RBF` kernels, varying `C` in\n",
    "`[0.1, 1.0, 10]`, varying gamma in `[0.0001, 0.01, 0.1]`)\n",
    "- Artificial Neural Networks (with one and two hidden layers)\n",
    "- Decision Trees (with varying `max depth`, `split` strategy and `quality measure`)\n",
    "- Naïve Bayes (with varying `var_smoothing`)\n",
    "- Logistic Regression (with `lbfgs` and `saga` solvers)\n",
    "\n",
    "And the performance metric used here is also the F1 score.\n",
    "\n",
    "As a reminder, the formula of the F1 score is:\n",
    "2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "However, in contrast to the paper, we use 5-fold cross validation during the grid\n",
    "search phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86548e4b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Hyperparameter search space\n",
    "\n",
    "classifier_hyperparameters = {\n",
    "    # K-Nearest Neighbours\n",
    "    KNeighborsClassifier: [\n",
    "        # {\"n_neighbors\": range(1, 51), \"weights\":[\"uniform\", \"distance\"]}\n",
    "        # We reduce search space for speed\n",
    "        {\n",
    "            \"n_neighbors\": [24],\n",
    "            \"weights\": [\"distance\"],\n",
    "        }\n",
    "    ],\n",
    "    # Support Vector Machines\n",
    "    SVC: [\n",
    "        # {\n",
    "        #     \"kernel\": [\"linear\"],\n",
    "        #     \"C\": [0.1, 1.0, 10],\n",
    "        #     \"probability\": [True],\n",
    "        #     \"random_state\": [RANDOM_STATE],\n",
    "        # },\n",
    "        {\n",
    "            \"kernel\": [\"rbf\"],\n",
    "            \"C\": [10],  # [0.1, 1.0, 10],\n",
    "            \"gamma\": [\"scale\"],  # [\"scale\", \"auto\", 0.0001, 0.01, 0.1],\n",
    "            \"probability\": [True],\n",
    "            \"random_state\": [RANDOM_STATE],\n",
    "        },\n",
    "    ],\n",
    "    # Artificial Neural Networks\n",
    "    MLPClassifier: [\n",
    "        {\n",
    "            \"max_iter\": [1000],\n",
    "            \"validation_fraction\": [0.2],\n",
    "            \"hidden_layer_sizes\": [(10,)],  # [(10,), (20,), (52, 10)],\n",
    "            \"random_state\": [RANDOM_STATE],\n",
    "            # [(i,) for i in range(2, 100, 10)] + [\n",
    "            #     (i, j) for i in range(2, 100, 10) for j in range(2, 100, 10)\n",
    "            # ],\n",
    "            # As we do not notice any improvement by varying `activation` and `alpha`,\n",
    "            # we choose to keep the default values for these parameters.\n",
    "            # \"activation\": [\"logistic\", \"tanh\", \"relu\"],\n",
    "            # \"alpha\": 10.0 ** (- np.arange(-1,6))\n",
    "        },\n",
    "    ],\n",
    "    # Decision Tree\n",
    "    DecisionTreeClassifier: [\n",
    "        {\n",
    "            \"criterion\": [\"entropy\"],  # [\"gini\", \"entropy\"],\n",
    "            \"splitter\": [\"best\"],  # [\"best\", \"random\"],\n",
    "            \"max_depth\": [6],  # [None, *list(range(1, 11))],\n",
    "            \"min_samples_split\": [2],  # range(2, 11, 2),\n",
    "            \"min_samples_leaf\": [10],  # range(2, 11, 2),\n",
    "            \"random_state\": [RANDOM_STATE],\n",
    "        },\n",
    "    ],\n",
    "    # Naive Bayes\n",
    "    GaussianNB: [\n",
    "        {\n",
    "            \"var_smoothing\": [1e-9],  # [1e-9, 1e-8, 1e-7, 1e-6]\n",
    "        }\n",
    "    ],\n",
    "    # Logistic Regression\n",
    "    LogisticRegression: [\n",
    "        {\n",
    "            \"solver\": [\"lbfgs\"],  # [\"lbfgs\", \"saga\"],\n",
    "            \"random_state\": [RANDOM_STATE],\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "for classifier, hyperparameters in classifier_hyperparameters.items():\n",
    "    gs_classifier = GridSearchCV(classifier(), hyperparameters, scoring=\"f1\", n_jobs=-1)\n",
    "    gs_classifier.fit(x_train, y_train)\n",
    "    print(\n",
    "        f\"{classifier.__name__}: score={gs_classifier.score(x_test, y_test):.4f} \"\n",
    "        f\"best_parameters={gs_classifier.best_params_}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad52334b",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Classification at different points in time\n",
    "\n",
    "Predicting student final exam outcome seems to be more valuable at an early stage of\n",
    "the course as it might give instuctors more time to help the students at risk.\n",
    "However, predicting early is more challenging as less data is available for the\n",
    "classifiers.\n",
    "\n",
    "As in the work of Tomasevic et al., we will compare the classification performances at\n",
    "different moments of the course based on the number of assessments passed.\n",
    "\n",
    "Let's start by taking a look at the assessment table for the selected courses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07a3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "oulad.assessments[\n",
    "    (oulad.assessments.code_module == \"DDD\")\n",
    "    & (oulad.assessments.assessment_type == \"TMA\")\n",
    "    & (\n",
    "        (oulad.assessments.code_presentation == \"2013J\")\n",
    "        | (oulad.assessments.code_presentation == \"2014B\")\n",
    "    )\n",
    "].sort_values(\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d142d8",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We note that each course module has six intermediary assessments.\n",
    "\n",
    "Next, we use the final submisssion `date` field to filter out assessment related\n",
    "information after a given date and repeat the same data preprocessing and\n",
    "classification process as done previously.\n",
    "\n",
    "We also add Voting and MultiCons ensemble methods to check whether they might improve\n",
    "current results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d840e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = {}\n",
    "# We select the date such as both courses include the same amount of assessments\n",
    "# after the filter.\n",
    "for day in [25, 53, 88, 123, 165, 207]:\n",
    "    result[day] = []\n",
    "    feature_table = pd.concat(\n",
    "        [\n",
    "            get_feature_table(day),\n",
    "            get_feature_table(day, code_presentation=\"2014B\"),\n",
    "        ]\n",
    "    )\n",
    "    feature_table = fill_nas(feature_table)\n",
    "    x_train, x_test, y_train, y_test = normalized_train_test_split(feature_table)\n",
    "    print(f\"Computing classification results at day={day:.0f}...\")\n",
    "\n",
    "    train_predictions = []\n",
    "    predictions = []\n",
    "    estimators = []\n",
    "    for classifier, hyperparameters in classifier_hyperparameters.items():\n",
    "        gs_classifier = GridSearchCV(\n",
    "            classifier(), hyperparameters, scoring=\"f1\", n_jobs=-1\n",
    "        )\n",
    "        gs_classifier.fit(x_train, y_train)\n",
    "        estimators.append((classifier.__name__, gs_classifier))\n",
    "        predictions.append(gs_classifier.predict(x_test))\n",
    "        train_predictions.append(gs_classifier.predict(x_train))\n",
    "        result[day].append(round(f1_score(y_test, predictions[-1]), 4))\n",
    "\n",
    "    # Voting Classifier\n",
    "    voting = VotingClassifier(estimators=estimators, voting=\"soft\")\n",
    "    voting.fit(x_train, y_train)\n",
    "    result[day].append(round(f1_score(y_test, voting.predict(x_test)), 4))\n",
    "\n",
    "    # MultiCons\n",
    "    multicons_options = {\n",
    "        \"similarity_measure\": \"JaccardIndex\",\n",
    "        \"optimize_label_names\": True,\n",
    "        \"consensus_function\": \"consensus_function_12\",\n",
    "    }\n",
    "    # Searching for the best merging_threshold.\n",
    "    max_score = 0  # pylint: disable=invalid-name\n",
    "    merging_threshold = -1  # pylint: disable=invalid-name\n",
    "    for mt in np.arange(0, 1, 0.05):\n",
    "        recommended_consensus = (\n",
    "            MultiCons(**multicons_options, merging_threshold=mt)\n",
    "            .fit(train_predictions)\n",
    "            .labels_.astype(bool)\n",
    "        )\n",
    "        score = f1_score(y_train, recommended_consensus)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            merging_threshold = mt\n",
    "    print(f\"MultiCons: selected merging_threshold={merging_threshold:0.2f}\")\n",
    "\n",
    "    recommended_consensus = (\n",
    "        MultiCons(**multicons_options, merging_threshold=merging_threshold)\n",
    "        .fit(predictions)\n",
    "        .labels_.astype(bool)\n",
    "    )\n",
    "    result[day].append(round(f1_score(y_test, recommended_consensus), 4))\n",
    "\n",
    "\n",
    "classifier_names = [\n",
    "    classifier.__name__ for classifier in classifier_hyperparameters\n",
    "] + [\"Voting\", \"MultiCons\"]\n",
    "\n",
    "result_df = pd.DataFrame(result, index=classifier_names)\n",
    "print(\"\\nF1 score at different points in time:\")\n",
    "display(result_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
