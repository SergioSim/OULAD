{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering of OULAD dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "from sklearn.impute import SimpleImputer\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pylab as pl\n",
    "import subprocess\n",
    "import itertools\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import Birch\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sns.set()\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Charging the dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessments_df         = pd.read_csv('./OULAD/assessments.csv')\n",
    "courses_df             = pd.read_csv('./OULAD/courses.csv')\n",
    "studentAssessment_df   = pd.read_csv('./OULAD/studentAssessment.csv')\n",
    "studentInfo_df         = pd.read_csv('./OULAD/studentInfo.csv')\n",
    "studentRegistration_df = pd.read_csv('./OULAD/studentRegistration.csv')\n",
    "studentVle_df          = pd.read_csv('./OULAD/studentVle.csv')\n",
    "vle_df                 = pd.read_csv('./OULAD/vle.csv')\n",
    "\n",
    "# we store references\n",
    "dataset_dict = {\n",
    "    'assessments': assessments_df,\n",
    "    'courses': courses_df,\n",
    "    'studentAssessment': studentAssessment_df,\n",
    "    'studentInfo': studentInfo_df,\n",
    "    'studentRegistration': studentRegistration_df,\n",
    "    'studentVle': studentVle_df,\n",
    "    'vle': vle_df\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling NaNs / Filtering / Restructuring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some of imd_band are NaN -> replacing them with 0-100%\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='0-100%')\n",
    "studentInfo_df['imd_band'] = imp.fit_transform(studentInfo_df[['imd_band']])\n",
    "\n",
    "# 82% of vle.week_from vle.week_to are NaN\n",
    "# of the remaining 18% rows - in 99.2% of cases vle.week_from is equal to vle.week_to\n",
    "# with this in mind - we can ignore this data\n",
    "del vle_df['week_from']\n",
    "del vle_df['week_to']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering out one specific course and merging all tables into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getOneCourse(dataset_dict, code_module, code_presentation):\n",
    "    '''\n",
    "    Merge all tables by their primary keys for a single course,\n",
    "    replace NaNs of missing Exam/unregistration dates with module_presentation_length\n",
    "    '''\n",
    "    def filterByCode(table):\n",
    "        '''returns a boolean pd.Series of same table length'''\n",
    "        return  (table['code_module'] == code_module) & \\\n",
    "                (table['code_presentation'] == code_presentation)\n",
    "    \n",
    "    course = dataset_dict['courses']\n",
    "    course = course[filterByCode(course)]\n",
    "    module_presentation_length = course['module_presentation_length'].values[0]\n",
    "    \n",
    "    # studentAssessment -> complete studentAssessments with assessments info\n",
    "    assessments = dataset_dict['assessments']\n",
    "    assessments.loc[ (assessments['assessment_type'] == 'Exam') & \\\n",
    "        filterByCode(assessments), \\\n",
    "        'date'] = module_presentation_length\n",
    "    assessments = assessments[filterByCode(assessments)]\n",
    "    studentAssessment = pd.merge(dataset_dict['studentAssessment'], assessments, \\\n",
    "                                 how='inner', on='id_assessment')\n",
    "\n",
    "    # studentInfo -> complete studentInfo with studentRegistration\n",
    "    studentRegistration = dataset_dict['studentRegistration']\n",
    "    studentRegistration.loc[studentRegistration['date_unregistration'].isna() & \\\n",
    "        filterByCode(studentRegistration), \\\n",
    "        'date_unregistration'] = module_presentation_length\n",
    "    studentRegistration = studentRegistration[ \\\n",
    "                           filterByCode(studentRegistration)]\n",
    "    studentInfo = pd.merge(dataset_dict['studentInfo'], studentRegistration, \\\n",
    "                           how='inner', on=['id_student', 'code_module', 'code_presentation'])\n",
    "\n",
    "    # studentVle = complete vle with studentVle\n",
    "    vle = dataset_dict['vle']\n",
    "    vle = vle[filterByCode(vle)]\n",
    "    studentVle = pd.merge(dataset_dict['studentVle'], vle, \\\n",
    "                          how='inner', on=['id_site', 'code_module', 'code_presentation'])\n",
    "    del studentVle['id_site']\n",
    "    \n",
    "    # complete with studentVle with studentInfo, complete studentAssessment with studentInfo\n",
    "    # then append enhanced studentVle with studentAssessment, sort by date\n",
    "    studentVleInfo = pd.merge(studentVle, studentInfo, \\\n",
    "                              how='inner', on=['id_student', 'code_module', 'code_presentation'])\n",
    "    studentAssessmentInfo = pd.merge(studentAssessment, studentInfo, \\\n",
    "                              how='inner', on=['id_student', 'code_presentation', 'code_module'])\n",
    "    \n",
    "    combined_df = studentAssessmentInfo.append(studentVleInfo)\n",
    "    del combined_df['code_module']\n",
    "    del combined_df['code_presentation']\n",
    "    del combined_df['id_assessment']\n",
    "    combined_df = combined_df.sort_values(by=['date'])\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restructuring the oneCourse table\n",
    "- oneCourse table might be good for algorithms that take into account the sequence / time\n",
    "- to make a first POC we will simplify drasticaly\n",
    "    - remove every thing that is beyond day 14\n",
    "    - aggregate each student sequence to make for each student contain only one row\n",
    "        - date_submitted (ignore NaNs) -> compute the mean \n",
    "        - same with date, score\n",
    "        - is_banked (ignore NaNs) -> compute the sum (= how many assessments were banked)\n",
    "        - same with score\n",
    "        - assessment_type -> create 2 variables: \n",
    "            - sumTMA -> sum of TMAs\n",
    "            - sumCMA -> sum of CMAs\n",
    "        - for each activity type make a variable and take the sum of sum_click"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure(oneCourse):\n",
    "    '''\n",
    "    aggregate each student sequence to make for each student contain only one row,\n",
    "    keeping only the data of the first two weeks\n",
    "    '''\n",
    "    # prediction is only then interresting when it's the begining of the course - not the end!\n",
    "    first14Days_oneCourse = oneCourse[oneCourse['date'] <= 14]\n",
    "    # remove those who unregistered before the begining as we can't do anything for them\n",
    "    first14Days_oneCourse = first14Days_oneCourse[first14Days_oneCourse['date_unregistration'] \\\n",
    "                                                  > 0]\n",
    "    # list of unique activity_types to create features\n",
    "    activity_types_df = first14Days_oneCourse['activity_type'].unique()\n",
    "    # remove NaN activity type\n",
    "    activity_types_df = [x for x in activity_types_df if type(x) == str]\n",
    "    # we want one student per line (the easy way)\n",
    "    final_df = first14Days_oneCourse.groupby('id_student').agg({\n",
    "        'score': [np.mean, np.sum],\n",
    "        'date_submitted': [np.mean],\n",
    "        'is_banked': [np.sum],\n",
    "        'assessment_type': [('CMA_count', lambda x: x.values[x.values == 'CMA'].size), \\\n",
    "                            ('TMA_count', lambda x: x.values[x.values == 'TMA'].size)],\n",
    "        'date':[np.mean],\n",
    "        'weight': [np.mean, np.sum],\n",
    "        'gender': [('first', lambda x: x.values[0])],\n",
    "        'region': [('first', lambda x: x.values[0])],\n",
    "        'highest_education': [('first', lambda x: x.values[0])],\n",
    "        'imd_band': [('first', lambda x: x.values[0])],\n",
    "        'age_band': [('first', lambda x: x.values[0])],\n",
    "        'num_of_prev_attempts': [('first', lambda x: x.values[0])],\n",
    "        'studied_credits': [('first', lambda x: x.values[0])],\n",
    "        'disability': [('first', lambda x: x.values[0])],\n",
    "        'final_result': [('first', lambda x: x.values[0])],\n",
    "        'date_registration': [('first', lambda x: x.values[0])],\n",
    "        'date_unregistration': [('first', lambda x: x.values[0])],\n",
    "        'sum_click': [np.mean, np.sum],\n",
    "        'activity_type': [('list', lambda x: [x.values[x.values == activity].size \\\n",
    "                                              for activity in activity_types_df])]\n",
    "    })\n",
    "    # keeping only one level of columns names\n",
    "    final_df.columns = [\"_\".join(x) for x in final_df.columns.ravel()]\n",
    "    custom_columns = ['activity_type_' + str(x) for x in activity_types_df]\n",
    "    # splitting & concatenating the created features (there should be a better way to do it...)\n",
    "    final_df = final_df.join(pd.DataFrame(final_df.activity_type_list.values.tolist(), \\\n",
    "                                          columns=custom_columns, index=final_df.index))\n",
    "    del final_df['activity_type_list']\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace NaNs introduced by new features\n",
    "#### and map categorical_columns to numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanAndMap(final_df):\n",
    "    '''\n",
    "    replace NaNs intoduced by new features and map catogorical columns to numbers\n",
    "    returning the encoder object to decode the labels later on\n",
    "    '''\n",
    "    # replacing NaNs\n",
    "    final_df.loc[final_df['weight_mean'].isna(), 'weight_mean'] = 0\n",
    "    final_df.loc[final_df['score_mean'].isna(), 'score_mean'] = 0\n",
    "    final_df.loc[final_df['date_submitted_mean'].isna(), 'date_submitted_mean'] = -1\n",
    "\n",
    "    # replacing categorical_columns with numbers\n",
    "    categorical_columns = ['gender_first', 'highest_education_first', 'imd_band_first', \\\n",
    "                           'age_band_first', 'disability_first', 'final_result_first',  \\\n",
    "                           'region_first']\n",
    "\n",
    "    encoders = {col: LabelEncoder() for col in categorical_columns }\n",
    "    for col in categorical_columns:\n",
    "        final_df.loc[:, col] = encoders[col].fit_transform(final_df[col])\n",
    "    \n",
    "    return encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       score_mean  score_sum  date_submitted_mean  is_banked_sum  \\\n",
      "count      1577.0     1577.0               1577.0         1577.0   \n",
      "mean          0.0        0.0                 -1.0            0.0   \n",
      "std           0.0        0.0                  0.0            0.0   \n",
      "min           0.0        0.0                 -1.0            0.0   \n",
      "25%           0.0        0.0                 -1.0            0.0   \n",
      "50%           0.0        0.0                 -1.0            0.0   \n",
      "75%           0.0        0.0                 -1.0            0.0   \n",
      "max           0.0        0.0                 -1.0            0.0   \n",
      "\n",
      "       assessment_type_CMA_count  assessment_type_TMA_count    date_mean  \\\n",
      "count                     1577.0                     1577.0  1577.000000   \n",
      "mean                         0.0                        0.0     1.723750   \n",
      "std                          0.0                        0.0     5.068600   \n",
      "min                          0.0                        0.0   -18.000000   \n",
      "25%                          0.0                        0.0    -1.117647   \n",
      "50%                          0.0                        0.0     2.028736   \n",
      "75%                          0.0                        0.0     4.928571   \n",
      "max                          0.0                        0.0    14.000000   \n",
      "\n",
      "       weight_mean  weight_sum  gender_first  region_first  \\\n",
      "count       1577.0      1577.0   1577.000000   1577.000000   \n",
      "mean           0.0         0.0      0.769182      5.850349   \n",
      "std            0.0         0.0      0.421490      3.603675   \n",
      "min            0.0         0.0      0.000000      0.000000   \n",
      "25%            0.0         0.0      1.000000      3.000000   \n",
      "50%            0.0         0.0      1.000000      6.000000   \n",
      "75%            0.0         0.0      1.000000      9.000000   \n",
      "max            0.0         0.0      1.000000     12.000000   \n",
      "\n",
      "       highest_education_first  imd_band_first  age_band_first  \\\n",
      "count              1577.000000     1577.000000     1577.000000   \n",
      "mean                  0.939125        5.221940        0.287888   \n",
      "std                   1.016412        3.087999        0.470783   \n",
      "min                   0.000000        0.000000        0.000000   \n",
      "25%                   0.000000        3.000000        0.000000   \n",
      "50%                   1.000000        5.000000        0.000000   \n",
      "75%                   2.000000        8.000000        1.000000   \n",
      "max                   4.000000       10.000000        2.000000   \n",
      "\n",
      "       num_of_prev_attempts_first  studied_credits_first  disability_first  \\\n",
      "count                      1577.0            1577.000000       1577.000000   \n",
      "mean                          0.0              72.079899          0.084971   \n",
      "std                           0.0              38.156503          0.278928   \n",
      "min                           0.0              30.000000          0.000000   \n",
      "25%                           0.0              60.000000          0.000000   \n",
      "50%                           0.0              60.000000          0.000000   \n",
      "75%                           0.0              90.000000          0.000000   \n",
      "max                           0.0             630.000000          1.000000   \n",
      "\n",
      "       final_result_first  date_registration_first  date_unregistration_first  \\\n",
      "count         1577.000000              1577.000000                1577.000000   \n",
      "mean             1.913760               -83.535193                 183.431833   \n",
      "std              1.030706                68.901139                  85.421034   \n",
      "min              0.000000              -311.000000                   3.000000   \n",
      "25%              1.000000              -127.000000                 111.000000   \n",
      "50%              2.000000               -58.000000                 241.000000   \n",
      "75%              3.000000               -25.000000                 241.000000   \n",
      "max              3.000000                12.000000                 241.000000   \n",
      "\n",
      "       sum_click_mean  sum_click_sum  activity_type_subpage  \\\n",
      "count     1577.000000    1577.000000            1577.000000   \n",
      "mean         3.173519     197.194039              11.628408   \n",
      "std          1.768327     268.921172              15.666946   \n",
      "min          1.000000       1.000000               0.000000   \n",
      "25%          2.000000      40.000000               2.000000   \n",
      "50%          2.785714     110.000000               7.000000   \n",
      "75%          3.928571     253.000000              16.000000   \n",
      "max         18.625000    3907.000000             252.000000   \n",
      "\n",
      "       activity_type_url  activity_type_homepage  activity_type_resource  \\\n",
      "count        1577.000000             1577.000000             1577.000000   \n",
      "mean            2.140774                9.117945                8.086240   \n",
      "std             2.582245                6.581877                8.505085   \n",
      "min             0.000000                0.000000                0.000000   \n",
      "25%             1.000000                4.000000                2.000000   \n",
      "50%             1.000000                8.000000                6.000000   \n",
      "75%             3.000000               13.000000               11.000000   \n",
      "max            32.000000               33.000000               98.000000   \n",
      "\n",
      "       activity_type_oucontent  activity_type_quiz  activity_type_forumng  \\\n",
      "count              1577.000000         1577.000000            1577.000000   \n",
      "mean                  3.268865           10.099556               7.840837   \n",
      "std                   3.753002           12.650549              13.707168   \n",
      "min                   0.000000            0.000000               0.000000   \n",
      "25%                   0.000000            1.000000               0.000000   \n",
      "50%                   2.000000            6.000000               2.000000   \n",
      "75%                   5.000000           14.000000               9.000000   \n",
      "max                  23.000000          141.000000              92.000000   \n",
      "\n",
      "       activity_type_oucollaborate  \n",
      "count                  1577.000000  \n",
      "mean                      0.139505  \n",
      "std                       0.447291  \n",
      "min                       0.000000  \n",
      "25%                       0.000000  \n",
      "50%                       0.000000  \n",
      "75%                       0.000000  \n",
      "max                       4.000000  \n",
      "REGION REPARTITION\n",
      "region_first\n",
      "0     156\n",
      "1      97\n",
      "2      58\n",
      "3     151\n",
      "4     111\n",
      "5     134\n",
      "6     235\n",
      "7      82\n",
      "8     149\n",
      "9      95\n",
      "10     81\n",
      "11    131\n",
      "12     97\n",
      "dtype: int64\n",
      "EDUCATION REPARTITION\n",
      "highest_education_first\n",
      "0    712\n",
      "1    355\n",
      "2    451\n",
      "3     12\n",
      "4     47\n",
      "dtype: int64\n",
      "FINAL RESULT REPARTITION\n",
      "3    583\n",
      "2    467\n",
      "1    335\n",
      "0    192\n",
      "Name: final_result_first, dtype: int64\n",
      "final_df contains 0 NaNs\n"
     ]
    }
   ],
   "source": [
    "code_module = 'CCC'\n",
    "code_presentation = '2014B'\n",
    "oneCourse = getOneCourse(dataset_dict, code_module, code_presentation)\n",
    "final_df = restructure(oneCourse)\n",
    "encoders = cleanAndMap(final_df)\n",
    "\n",
    "# some information about the final_df\n",
    "print(final_df.describe(include='all'))\n",
    "print('REGION REPARTITION')\n",
    "print(final_df.groupby('region_first').size())\n",
    "print('EDUCATION REPARTITION')\n",
    "print(final_df.groupby('highest_education_first').size())\n",
    "print('FINAL RESULT REPARTITION')\n",
    "print(final_df['final_result_first'].value_counts())\n",
    "# to check that we don't have NaNs!\n",
    "print('final_df contains %i NaNs' % sum(final_df.isna().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "\n",
    "#### IDEA:\n",
    "- imagine the course just started - we are at week 2\n",
    "- we want to identify students which might Fail/Withdraw to propose them help\n",
    "- to start of simple we don't try to predict WHEN a student will Fail/Withdraw and we ignore all student interactions with other courses\n",
    "- we suppose that students that Fail/Withdraw can be distinguished by their behaviour and features but we don't know which ones in advance\n",
    "- with clustering we can identify groups of student that are similar\n",
    "- we suppose that students that have similar behaviour / features would have similar outcomes\n",
    "\n",
    "- we want to measure if the resulting groups could provide useful information for a set of prediction algorithms thereby improving their predictions \n",
    "\n",
    "- how does the quality of the generated groups impact the predictors?\n",
    "\n",
    "- could a consensus clustering algorithm produce an even better set of groups to improve even more the prediction?\n",
    "- how does the clustering algorithms generalise for differenes courses / the same course the next year?\n",
    "- if each time the optimal hyper params are different - could the consensus clustering \n",
    "   algorithm solve this issue by combining multiple choices of hyper params at the same time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separate response label from training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another solution would be to cluster a previous course (training) and use the current one for\n",
    "#   prediction ... for now we want just to measure the clustering quality on all the data\n",
    "# train, test = train_test_split(final_df, random_state=0)\n",
    "scaler = MinMaxScaler()\n",
    "trainX = final_df.drop(['final_result_first'], axis=1)\n",
    "trainX = scaler.fit_transform(trainX)\n",
    "testY = final_df['final_result_first']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom clustor quality measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customClusteringScore(encoders, clustAlgo_labels, testY):\n",
    "    new_col_list = encoders['final_result_first'].inverse_transform([0,1,2,3])\n",
    "    compare_df = pd.concat([testY, clustAlgo_labels], axis=1)\n",
    "    # bring back the final_result labels (not really needed but makes the code clearer)\n",
    "    compare_df.loc[:,'final_result_first'] = \\\n",
    "        encoders['final_result_first'].inverse_transform(compare_df.loc[:,'final_result_first'])\n",
    "    compare_df = compare_df.groupby([clustAlgo_labels.name, 'final_result_first']).size()\n",
    "    # the adjusted_rand_score measure is good for measuring the quality of the cluster\n",
    "    #   in general\n",
    "    # print('adjusted_rand_score: ', adjusted_rand_score(testY, kmeans_labels))\n",
    "\n",
    "    # but we want to measure how good the clustering algorithm is able to separate \n",
    "    #  Distinction/Pass from Fail/Withdrawn:\n",
    "    # the measure should be close or equal to 0 if both groups are almost equal or equal in size\n",
    "    # should be close or equal to 1 if one of the groups is small compared to the other\n",
    "    # should be between (0,1) if g1 > g2 / g1 < g2\n",
    "    # proposed measure: score = abs(g1 - g2) / g1 + g2\n",
    "    # we want to take into account the size of the groups too\n",
    "    # we prefer 'big' groups with a good score rather than small groups with exellent scores\n",
    "    # for that we could normalize the group score : score * group-size / total size\n",
    "    # a perfect clustering would be one that has only 2 groups each of them of score 1\n",
    "    # small groups would have a little weight compared to others but can sum up to 1\n",
    "    # for now let try not to penalize by group count...\n",
    "    nb_clusters = clustAlgo_labels.nunique()\n",
    "    scores = []\n",
    "    for cluster in range(0,nb_clusters):\n",
    "        failWihdrawn = compare_df[cluster]['Fail'] \\\n",
    "                            if 'Fail' in compare_df[cluster].index else 0\n",
    "        failWihdrawn += compare_df[cluster]['Withdrawn'] \\\n",
    "                            if 'Withdrawn' in compare_df[cluster].index else 0\n",
    "        PassDistinction = compare_df[cluster]['Pass'] \\\n",
    "                            if 'Pass' in compare_df[cluster].index else 0\n",
    "        PassDistinction += compare_df[cluster]['Distinction'] \\\n",
    "                            if 'Distinction' in compare_df[cluster].index else 0\n",
    "        scores.append(\\\n",
    "            (abs(PassDistinction - failWihdrawn) / (PassDistinction + failWihdrawn)) *\\\n",
    "                      (PassDistinction + failWihdrawn) / len(testY))\n",
    "    return sum(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Used clustering algorithms and their hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "kMedoidsParams = {\n",
    "    'vanilla' : {},\n",
    "    # removing to improve performances\n",
    "#     'init=k-medoids++': {'init': 'k-medoids++'},\n",
    "#     'metric=l1': {'metric': 'l1'},\n",
    "#     'metric=l2': {'metric': 'l2'},\n",
    "#     'metric=manhattan': {'metric': 'manhattan'}\n",
    "}\n",
    "\n",
    "kmeansParams = {\n",
    "    'vanilla' : {},\n",
    "    'tol=5': {'tol': 1e-5},\n",
    "     # removing to improve performance\n",
    "#     'n_init30' : {'n_init': 30},\n",
    "#     'max_iter=400': {'max_iter': 400},\n",
    "#     'max_iter=600': {'max_iter': 600},\n",
    "#     'max_iter=400 tol=5': {'max_iter': 400, 'tol': 1e-5},\n",
    "#     'max_iter=600 tol=5': {'max_iter': 600, 'tol': 1e-5},\n",
    "#     'max_iter=400 tol=6': {'max_iter': 400, 'tol': 1e-6},\n",
    "#     'max_iter=600 tol=6': {'max_iter': 600, 'tol': 1e-6},\n",
    "#     'n_init15 max_iter=400' : {'n_init': 15, 'max_iter': 400},\n",
    "#     'n_init15 max_iter=600' : {'n_init': 15, 'max_iter': 600},\n",
    "    \n",
    "    # removing because of low/similar results\n",
    "#     'tol=6': {'tol': 1e-6},\n",
    "#     'n_init15' : {'n_init': 15},\n",
    "#     'n_init60' : {'n_init': 60},\n",
    "#     'max_iter=800': {'max_iter': 800},\n",
    "#     'max_iter=800 tol=5': {'max_iter': 800, 'tol': 1e-5},\n",
    "#     'max_iter=800 tol=6': {'max_iter': 800, 'tol': 1e-6},\n",
    "#     'n_init15 max_iter=800' : {'n_init': 15, 'max_iter': 800},\n",
    "#     'n_init30 max_iter=400' : {'n_init': 30, 'max_iter': 400},\n",
    "#     'n_init30 max_iter=600' : {'n_init': 30, 'max_iter': 600},\n",
    "#     'n_init30 max_iter=800' : {'n_init': 30, 'max_iter': 800},\n",
    "#     'n_init60 max_iter=400' : {'n_init': 60, 'max_iter': 400},\n",
    "#     'n_init60 max_iter=600' : {'n_init': 60, 'max_iter': 600},\n",
    "#     'n_init60 max_iter=800' : {'n_init': 60, 'max_iter': 800},\n",
    "}\n",
    "\n",
    "spectralClusteringParams = {\n",
    "    'affinity=laplacian': {'affinity': 'laplacian'},\n",
    "    # removing to improve performance\n",
    "#     'vanilla' : {},\n",
    "#     'eigen_solver=arpack': {'eigen_solver': 'arpack'},\n",
    "#     'eigen_solver=amg': {'eigen_solver': 'amg'},\n",
    "#     'assign_labels=discretize': {'assign_labels': 'discretize'},\n",
    "#     'eigen_solver=lobpcg': {'eigen_solver': 'lobpcg'}, # not working for k>=6\n",
    "}\n",
    "\n",
    "agglomerativeClusteringParams = {\n",
    "    'vanilla' : {},\n",
    "    \n",
    "    # removing to improve performance\n",
    "    'linkage=average affinity=l1' : {'linkage': 'average', 'affinity': 'l1'},\n",
    "    'linkage=average affinity=manhattan' : {'linkage': 'average', 'affinity': 'manhattan'},\n",
    "    \n",
    "    # removing because of low results\n",
    "    'linkage=complete' : {'linkage': 'complete'},\n",
    "    'linkage=average' : {'linkage': 'average'},\n",
    "    'linkage=single' : {'linkage': 'single'},\n",
    "    'linkage=complete affinity=l1' : {'linkage': 'complete', 'affinity': 'l1'},\n",
    "    'linkage=single affinity=l1' : {'linkage': 'single', 'affinity': 'l1'},\n",
    "    'linkage=complete affinity=l2' : {'linkage': 'complete', 'affinity': 'l2'},\n",
    "    'linkage=average affinity=l2' : {'linkage': 'average', 'affinity': 'l2'},\n",
    "    'linkage=single affinity=l2' : {'linkage': 'single', 'affinity': 'l2'},\n",
    "    'linkage=complete affinity=manhattan' : {'linkage': 'complete', 'affinity': 'manhattan'},\n",
    "    'linkage=single affinity=manhattan' : {'linkage': 'single', 'affinity': 'manhattan'},\n",
    "    'linkage=complete affinity=cosine' : {'linkage': 'complete', 'affinity': 'cosine'},\n",
    "    'linkage=average affinity=cosine' : {'linkage': 'average', 'affinity': 'cosine'},\n",
    "    'linkage=single affinity=cosine' : {'linkage': 'single', 'affinity': 'cosine'},\n",
    "}\n",
    "\n",
    "birchParams = {\n",
    "    'vanilla': {},\n",
    "}\n",
    "\n",
    "dbscanParams = {\n",
    "    'vanilla': {},\n",
    "}\n",
    "\n",
    "clustAlgos = {\n",
    "    \"kMeans\": {\n",
    "        'obj': KMeans, \n",
    "        'params': kmeansParams,\n",
    "        'paramRanges': {'n_clusters' : range(3, 4)}\n",
    "    },\n",
    "    \"kMedoits\": {\n",
    "        'obj': KMedoids, \n",
    "        'params': kMedoidsParams,\n",
    "        'paramRanges': {'n_clusters' : range(2, 4)}\n",
    "    },\n",
    "    \"SpectralClustering\": {\n",
    "        'obj': SpectralClustering,\n",
    "        'params': spectralClusteringParams,\n",
    "        'paramRanges': {'n_clusters' : range(3, 5)}\n",
    "    },\n",
    "#     \"AgglomerativeClustering\": {\n",
    "#         'obj': AgglomerativeClustering,\n",
    "#         'params': agglomerativeClusteringParams,\n",
    "#         'paramRanges': {'n_clusters' : range(2, 4)}\n",
    "#     },\n",
    "#     \"Birch\": {\n",
    "#         'obj': Birch, \n",
    "#         'params': birchParams,\n",
    "#         'paramRanges': {'n_clusters' : range(2, 4)}\n",
    "#     },\n",
    "    \n",
    "#     \"DBSCAN\": {'obj': DBSCAN, 'params': dbscanParams}, # density based - don't use nb_clusters\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing the baseClusterings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 6/6 (100%)\n",
      "            1  2  3  4  5  6\n",
      "id_student                  \n",
      "28418       2  0  0  0  1  3\n",
      "29764       0  1  1  1  2  1\n",
      "29820       0  1  1  1  2  1\n",
      "40333       1  2  1  2  0  2\n",
      "40604       0  1  1  1  2  1\n",
      "...        .. .. .. .. .. ..\n",
      "2681198     1  2  1  2  0  2\n",
      "2686578     0  1  0  1  0  1\n",
      "2692327     0  1  1  1  2  1\n",
      "2697181     2  0  0  0  1  3\n",
      "2698535     0  1  0  0  2  1\n",
      "\n",
      "[1577 rows x 6 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SpectralClustering_k</th>\n",
       "      <th>SpectralClustering_type</th>\n",
       "      <th>SpectralClustering_score</th>\n",
       "      <th>kMeans_k</th>\n",
       "      <th>kMeans_type</th>\n",
       "      <th>kMeans_score</th>\n",
       "      <th>kMedoits_k</th>\n",
       "      <th>kMedoits_type</th>\n",
       "      <th>kMedoits_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>affinity=laplacian</td>\n",
       "      <td>0.398859</td>\n",
       "      <td>3</td>\n",
       "      <td>tol=5</td>\n",
       "      <td>0.305010</td>\n",
       "      <td>3</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>0.360812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>affinity=laplacian</td>\n",
       "      <td>0.302473</td>\n",
       "      <td>3</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>0.303741</td>\n",
       "      <td>2</td>\n",
       "      <td>vanilla</td>\n",
       "      <td>0.242866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  SpectralClustering_k SpectralClustering_type  SpectralClustering_score  \\\n",
       "0                    3      affinity=laplacian                  0.398859   \n",
       "1                    4      affinity=laplacian                  0.302473   \n",
       "2                  NaN                     NaN                       NaN   \n",
       "3                  NaN                     NaN                       NaN   \n",
       "4                  NaN                     NaN                       NaN   \n",
       "5                  NaN                     NaN                       NaN   \n",
       "6                  NaN                     NaN                       NaN   \n",
       "7                  NaN                     NaN                       NaN   \n",
       "8                  NaN                     NaN                       NaN   \n",
       "9                  NaN                     NaN                       NaN   \n",
       "\n",
       "  kMeans_k kMeans_type  kMeans_score kMedoits_k kMedoits_type  kMedoits_score  \n",
       "0        3       tol=5      0.305010          3       vanilla        0.360812  \n",
       "1        3     vanilla      0.303741          2       vanilla        0.242866  \n",
       "2      NaN         NaN           NaN        NaN           NaN             NaN  \n",
       "3      NaN         NaN           NaN        NaN           NaN             NaN  \n",
       "4      NaN         NaN           NaN        NaN           NaN             NaN  \n",
       "5      NaN         NaN           NaN        NaN           NaN             NaN  \n",
       "6      NaN         NaN           NaN        NaN           NaN             NaN  \n",
       "7      NaN         NaN           NaN        NaN           NaN             NaN  \n",
       "8      NaN         NaN           NaN        NaN           NaN             NaN  \n",
       "9      NaN         NaN           NaN        NaN           NaN             NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 Generate clustering ensemble of the dataset and store the clustering vectors in a \n",
    "#   list BaseClusterings\n",
    "baseClusterings = pd.DataFrame(index=testY.index)\n",
    "results = pd.DataFrame(columns=['k', 'algo', 'type', 'score'])\n",
    "stepCount = 0\n",
    "totalStepsCount = sum([len(clustAlgos[key]['params']) * \\\n",
    "                       sum([len(clustAlgos[key]['paramRanges'][innerkey]) \\\n",
    "                            for innerkey in clustAlgos[key]['paramRanges']]) \\\n",
    "                            for key in clustAlgos])\n",
    "\n",
    "for clustAlgoKey in clustAlgos:\n",
    "    for paramsKey in clustAlgos[clustAlgoKey]['params']:\n",
    "        for rangedParamKey in clustAlgos[clustAlgoKey]['paramRanges']:\n",
    "            for rangedParamValue in clustAlgos[clustAlgoKey]['paramRanges'][rangedParamKey]:\n",
    "                # for for for for for for for for ...\n",
    "                clear_output(wait=True)\n",
    "                clustAlgos[clustAlgoKey]['params'][paramsKey][rangedParamKey] = \\\n",
    "                                rangedParamValue\n",
    "                clustAlgo = clustAlgos[clustAlgoKey]['obj'](\\\n",
    "                                **clustAlgos[clustAlgoKey]['params'][paramsKey])\n",
    "                clustAlgo.fit(trainX)\n",
    "                # we don't want to predict - for now only evaluate the quality of the clustering\n",
    "                # clustAlgo_predict = pd.Series(clustAlgo.predict(trainX), name=clustAlgoKey, \\\n",
    "                # index=testY.index)\n",
    "                clustAlgo_labels = pd.Series(clustAlgo.labels_, name=clustAlgoKey, \\\n",
    "                                             index=testY.index)\n",
    "                baseClusterings.insert(len(baseClusterings.columns), str(stepCount+1), \\\n",
    "                                       clustAlgo_labels)\n",
    "\n",
    "                results = results.append({\n",
    "                    'k': rangedParamValue, \n",
    "                    'algo': clustAlgoKey, \n",
    "                    'type': paramsKey, \n",
    "                    'score': customClusteringScore(encoders, clustAlgo_labels, testY) \n",
    "                    }, ignore_index=True)\n",
    "                stepCount += 1\n",
    "                print('step %i/%i (%i%s)' % (stepCount, totalStepsCount, \\\n",
    "                                             round(100 * stepCount/totalStepsCount,2), '%'))\n",
    "\n",
    "nb_lines = 10\n",
    "resultTable = pd.DataFrame(index=range(0,nb_lines))\n",
    "for group in results.sort_values(by='score', ascending=False).groupby(['algo']):\n",
    "    if len(group[1]) < nb_lines:\n",
    "        group = (group[0], group[1].append([x for x in [0] * nb_lines]))\n",
    "    \n",
    "    resultTable.insert(len(resultTable.columns), \"%s_k\" % \\\n",
    "                       (group[0]), group[1].head(nb_lines)['k'].values)\n",
    "    resultTable.insert(len(resultTable.columns), \"%s_type\" % \\\n",
    "                       (group[0]), group[1].head(nb_lines)['type'].values)\n",
    "    resultTable.insert(len(resultTable.columns), \"%s_score\" % \\\n",
    "                       (group[0]), group[1].head(nb_lines)['score'].values)\n",
    "\n",
    "print(baseClusterings)\n",
    "resultTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MultiCons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# D= {1,2,3,4,5,6,7,8,9} partitioned using five base clusterings into the five partitions:\n",
    "# P1 = {{1,2,3},{4,5,6,7,8,9}},\n",
    "# P2 = {{1,2,3},{4,5,6,7,8,9}},\n",
    "# P3 = {{1,2,3,4,5},{6,7},{8,9}},\n",
    "# P4 = {{4,5,6,7}, {1,2,3},{8,9}}\n",
    "# P5 = {{4,5,6,7},{1,2,3},{8,9}}\n",
    "clust = pd.DataFrame(index=range(1,10))\n",
    "membershipMatrix = pd.DataFrame(index=range(1,10))\n",
    "clust.insert(len(clust.columns), '1', [0,0,0,1,1,1,1,1,1])\n",
    "clust.insert(len(clust.columns), '2', [0,0,0,1,1,1,1,1,1])\n",
    "clust.insert(len(clust.columns), '3', [0,0,0,0,0,1,1,2,2])\n",
    "clust.insert(len(clust.columns), '4', [1,1,1,0,0,0,0,2,2])\n",
    "clust.insert(len(clust.columns), '5', [1,1,1,0,0,0,0,2,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseClusterings = clust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the Membership matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1P0</th>\n",
       "      <th>1P1</th>\n",
       "      <th>2P0</th>\n",
       "      <th>2P1</th>\n",
       "      <th>3P0</th>\n",
       "      <th>3P1</th>\n",
       "      <th>3P2</th>\n",
       "      <th>4P0</th>\n",
       "      <th>4P1</th>\n",
       "      <th>4P2</th>\n",
       "      <th>5P0</th>\n",
       "      <th>5P1</th>\n",
       "      <th>5P2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   1P0  1P1  2P0  2P1  3P0  3P1  3P2  4P0  4P1  4P2  5P0  5P1  5P2\n",
       "1    1    0    1    0    1    0    0    0    1    0    0    1    0\n",
       "2    1    0    1    0    1    0    0    0    1    0    0    1    0\n",
       "3    1    0    1    0    1    0    0    0    1    0    0    1    0\n",
       "4    0    1    0    1    1    0    0    1    0    0    1    0    0\n",
       "5    0    1    0    1    1    0    0    1    0    0    1    0    0\n",
       "6    0    1    0    1    0    1    0    1    0    0    1    0    0\n",
       "7    0    1    0    1    0    1    0    1    0    0    1    0    0\n",
       "8    0    1    0    1    0    0    1    0    0    1    0    0    1\n",
       "9    0    1    0    1    0    0    1    0    0    1    0    0    1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# there should be a much better way to construct the membership matrix...\n",
    "# for now let's use a naive implementation\n",
    "\n",
    "# 3 Build the cluster membership matrix M\n",
    "def buildMembershipMatrix(baseClusterings):\n",
    "    ''' Computes and returns the Membership matrix'''\n",
    "    # colDict = {}\n",
    "    membershipMatrix = pd.DataFrame(index=baseClusterings.index)\n",
    "    for col in baseClusterings.columns:\n",
    "    #     colDict[col] = []\n",
    "        for partition in np.sort(baseClusterings[col].unique()):\n",
    "            membershipMatrix.insert(len(membershipMatrix.columns), '%sP%i' % (col, partition), baseClusterings[col] == partition)\n",
    "    #         colDict[col].append(partition)\n",
    "    #     colDict[col].append(-1)\n",
    "    # colDict[col].pop()\n",
    "    return membershipMatrix\n",
    "\n",
    "membershipMatrix = buildMembershipMatrix(baseClusterings)\n",
    "membershipMatrix.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate FCPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# me trying to mine frequent closed patterns... and resulting to mine all patterns... \n",
    "# result is very slow compared to existing FCI mining techniques\n",
    "# We will not use this function!\n",
    "def slowFCImining(): \n",
    "    def getPartitionFromArray(clust_array):\n",
    "        res = []\n",
    "        for i, val in enumerate(clust_array):\n",
    "            if colDict[str(i+1)][val] >= 0:\n",
    "                res.append(''.join([str(i+1), 'P', str(colDict[str(i+1)][val])]))\n",
    "\n",
    "        return res\n",
    "\n",
    "    def getKeyFromArray(clust_array):\n",
    "        return '|'.join(getPartitionFromArray(clust_array))\n",
    "\n",
    "    def checkAndInsertFrequentPattern(frequentPatterns, patternKeys, patternArray):\n",
    "        strhash = hash(str(patternArray))\n",
    "        if strhash in frequentPatterns:\n",
    "            if set(frequentPatterns[strhash][0]).issubset(patternKeys):\n",
    "                frequentPatterns[strhash] = [patternKeys, patternArray]\n",
    "\n",
    "        else:\n",
    "            frequentPatterns[strhash] = [patternKeys, patternArray]\n",
    "\n",
    "    iclust = 0\n",
    "    jpart = 0\n",
    "    temp_clust_array = []\n",
    "    memoryDict = {}\n",
    "    frequentPattersDict = {}\n",
    "    emptyJoin = False\n",
    "    start = time.time()\n",
    "\n",
    "    while True:\n",
    "        if str(iclust + 1) in colDict and not emptyJoin:\n",
    "            iclust += 1\n",
    "            jpart = 0\n",
    "            temp_clust_array.append(jpart)\n",
    "\n",
    "            if len(getPartitionFromArray(temp_clust_array)) == 1:\n",
    "                binaryJoin = membershipMatrix[getPartitionFromArray(temp_clust_array)[0]]\n",
    "                memoryDict[getKeyFromArray(temp_clust_array)] = binaryJoin\n",
    "                checkAndInsertFrequentPattern(frequentPattersDict, getPartitionFromArray(temp_clust_array),binaryJoin[binaryJoin].index.tolist())\n",
    "            elif len(getPartitionFromArray(temp_clust_array)) > 1:\n",
    "                binaryJoin = memoryDict[getKeyFromArray(temp_clust_array[:-1])] & membershipMatrix[getPartitionFromArray(temp_clust_array)[-1] ]\n",
    "                if sum(binaryJoin) != 0:\n",
    "                    checkAndInsertFrequentPattern(frequentPattersDict, getPartitionFromArray(temp_clust_array),binaryJoin[binaryJoin].index.tolist())\n",
    "                    if str(iclust + 1) in colDict:\n",
    "                        memoryDict[getKeyFromArray(temp_clust_array)] = binaryJoin\n",
    "                else:\n",
    "                    if str(iclust + 1) in colDict:\n",
    "                        emptyJoin = True\n",
    "\n",
    "        elif jpart < len(colDict[str(iclust)]) - 1 and not emptyJoin:\n",
    "            jpart += 1\n",
    "            temp_clust_array[len(temp_clust_array)-1] = jpart\n",
    "\n",
    "            if getKeyFromArray(temp_clust_array[:-1]) in memoryDict:\n",
    "                binaryJoin = memoryDict[getKeyFromArray(temp_clust_array[:-1])] & membershipMatrix[getPartitionFromArray(temp_clust_array)[-1] ]\n",
    "                if sum(binaryJoin) != 0:\n",
    "                    checkAndInsertFrequentPattern(frequentPattersDict, getPartitionFromArray(temp_clust_array),binaryJoin[binaryJoin].index.tolist())\n",
    "            else:\n",
    "                nojoin = membershipMatrix[getPartitionFromArray(temp_clust_array)[-1]]\n",
    "                checkAndInsertFrequentPattern(frequentPattersDict, getPartitionFromArray(temp_clust_array),binaryJoin[binaryJoin].index.tolist())\n",
    "\n",
    "        else:\n",
    "            lastKeyValue = {}\n",
    "            emptyJoin = False\n",
    "            while len(temp_clust_array) != 0 and \\\n",
    "                temp_clust_array[len(temp_clust_array) - 1] == len(colDict[str(iclust)]) - 1:\n",
    "                temp_clust_array.pop()\n",
    "                iclust -= 1\n",
    "\n",
    "                keyToDelete = getKeyFromArray(temp_clust_array)\n",
    "                if keyToDelete in memoryDict:\n",
    "                    lastKeyValue[keyToDelete] = memoryDict[keyToDelete]\n",
    "                    del memoryDict[keyToDelete]\n",
    "\n",
    "            if len(temp_clust_array) == 0:\n",
    "                print('END')\n",
    "                break;\n",
    "\n",
    "            temp_clust_array[len(temp_clust_array) - 1] += 1\n",
    "            if not temp_clust_array[len(temp_clust_array) - 1] == len(colDict[str(iclust)]) - 1:\n",
    "\n",
    "                ### dublicate as above\n",
    "                if len(getPartitionFromArray(temp_clust_array)) == 1:\n",
    "                    memoryDict = {}\n",
    "                    binaryJoin = membershipMatrix[getPartitionFromArray(temp_clust_array)[0]]\n",
    "                    memoryDict[getKeyFromArray(temp_clust_array)] = binaryJoin\n",
    "                    checkAndInsertFrequentPattern(frequentPattersDict, getPartitionFromArray(temp_clust_array),binaryJoin[binaryJoin].index.tolist())\n",
    "                elif len(getPartitionFromArray(temp_clust_array)) > 1:\n",
    "                    binaryJoin = memoryDict[getKeyFromArray(temp_clust_array[:-1])] & membershipMatrix[getPartitionFromArray(temp_clust_array)[-1] ]\n",
    "                    if sum(binaryJoin) != 0:\n",
    "                        checkAndInsertFrequentPattern(frequentPattersDict, getPartitionFromArray(temp_clust_array),binaryJoin[binaryJoin].index.tolist())\n",
    "                    if str(iclust + 1) in colDict:\n",
    "                        memoryDict[getKeyFromArray(temp_clust_array)] = binaryJoin\n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    print('Python elapsed time: ' + str(round((end - start), 3) * 1000) + ' ms')\n",
    "    for key in frequentPattersDict:\n",
    "        print(frequentPattersDict[key][0], len(frequentPattersDict[key][1]))\n",
    "        \n",
    "# slowFCImining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python elapsed time: 18.0 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['1P1', '2P1', '3P2', '4P2', '5P2'],\n",
       " ['1P0', '2P0', '3P0', '4P1', '5P1'],\n",
       " ['1P1', '2P1', '3P1', '4P0', '5P0'],\n",
       " ['1P1', '2P1', '3P0', '4P0', '5P0'],\n",
       " ['1P1', '2P1', '4P0', '5P0'],\n",
       " ['1P1', '2P1'],\n",
       " ['3P0']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The LCM algorithm is much faster!\n",
    "# thanks to https://github.com/slide-lig/plcmpp for the implementation of the LCM algorithm\n",
    "# It's cloned and build in the ./FCI directory\n",
    "\n",
    "start = time.time()\n",
    "groups = list(range(0,len(membershipMatrix.columns)))\n",
    "transactionList = [(membershipMatrix.iloc[x,:] * groups)[membershipMatrix.iloc[x,:]] for x in range(0, len(membershipMatrix))]\n",
    "\n",
    "file = open('./FCI/input.txt','w')\n",
    "for line in transactionList:\n",
    "    file.write(str(list(line)).replace('[', '').replace(',', '').replace(']', '') + '\\n')\n",
    "\n",
    "file.close()\n",
    "\n",
    "os.chdir(os.getcwd() + '/FCI')\n",
    "# 4 Generate FCPs from M for minsupport = 0\n",
    "subprocess.call(\"./runLCM.sh\")\n",
    "os.chdir(os.getcwd()[:-3])\n",
    "\n",
    "file = open('./FCI/output.txt','r')\n",
    "FCPs = []\n",
    "for line in file:\n",
    "    line = line.replace('\\n', '')\n",
    "    freq = line[:line.find('\t')]\n",
    "    line = np.array(list(map(int, line[line.find('\t')+1:].split(' '))))\n",
    "    line.sort()\n",
    "    FCPs.append(list(membershipMatrix.columns[line]))\n",
    "    \n",
    "file.close()\n",
    "end = time.time()\n",
    "print('Python elapsed time: ' + str(round((end - start), 3) * 1000) + ' ms')\n",
    "# 5 Sort the FCPs in ascending order according to the size of the instance sets\n",
    "FCPs.sort(key = len, reverse = True)\n",
    "FCPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consensusFunction10(biClust):\n",
    "    '''made from the Algorithm 10'''\n",
    "    hasChanged = True\n",
    "    while hasChanged:\n",
    "        # When hasChanged is False after the iteration\n",
    "        # All sets in biClust are unique\n",
    "        hasChanged = False\n",
    "        i = 0\n",
    "        N = len(biClust)\n",
    "        # using while loop because for i in range(1, N) would loop over a copy of \n",
    "        # 1,2,...,N => 1, 2,...N' and would not change if we change N\n",
    "        while i < N:\n",
    "            bi = biClust[i]\n",
    "            # as the intersection is a symetric operation we could omit half of\n",
    "            # the comparaisons? using j = i + 1 insead of j = 0\n",
    "            j = i + 1\n",
    "            while j < N:\n",
    "                # ommiting this part as with j = i + 1 we don't enter this if statement\n",
    "#                 if i == j:\n",
    "#                     j += 1\n",
    "#                     continue\n",
    "                bj = biClust[j]\n",
    "                intrscSz = bi & bj\n",
    "                if len(intrscSz) == 0:\n",
    "                    j += 1\n",
    "                elif len(intrscSz) == len(bi):\n",
    "                    # BiBj\n",
    "                    hasChanged = True\n",
    "                    del biClust[i]\n",
    "                    N -= 1\n",
    "                elif len(intrscSz) == len(bj):\n",
    "                    # BjBi\n",
    "                    hasChanged = True\n",
    "                    del biClust[j]\n",
    "                    N -= 1\n",
    "                else:\n",
    "                    biClust[j] = bi | bj\n",
    "                    hasChanged = True\n",
    "                    del biClust[i]\n",
    "                    N -= 1\n",
    "\n",
    "            i += 1\n",
    "\n",
    "def assignLabel(biClust, maxDT, consVctrs):\n",
    "    '''\n",
    "    8 - Assign a label to each set in BiClust to build the first consensus vector and \n",
    "    store it in a list of vectors ConsVctrs\n",
    "    * for convenience, ConsVctrs is a list of dictionaries that will be transformed later on \n",
    "    * in their corresponding consensus vectors\n",
    "    * storing {'maxDT=5|0': {1, 2, 3}, 'maxDT=5|1': {4, 5} ...\n",
    "    * instead of [0, 0, 0, 1, 1,...]\n",
    "    '''\n",
    "    temp = {}\n",
    "    for i, partition in enumerate(biClust):\n",
    "        temp['maxDT=%i|%i' % (maxDT,i)] = partition\n",
    "    consVctrs.append(temp)\n",
    "\n",
    "def jaccard(x, y):\n",
    "    '''\n",
    "    x and y are two dictionaries containing sets\n",
    "    returns the jaccard_score (|xy|/|xy|)\n",
    "    '''\n",
    "    xSet = [frozenset(x[key]) for key in x]        \n",
    "    ySet = [frozenset(y[key]) for key in y]\n",
    "    smallerOrEqualSet, biggerOrEqualSet = (xSet, ySet) if len(xSet) < len(ySet) else (ySet, xSet)\n",
    "    unionSet = set()\n",
    "    intersectionSet = set()\n",
    "    for i in range(0, len(biggerOrEqualSet)):\n",
    "        unionSet.add(biggerOrEqualSet[i])\n",
    "        if i < len(smallerOrEqualSet):\n",
    "            unionSet.add(biggerOrEqualSet[i] | smallerOrEqualSet[i])\n",
    "            intersectionSet.add(biggerOrEqualSet[i] & smallerOrEqualSet[i])\n",
    "    return len(intersectionSet) / len(unionSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stability=1 similarity=0.163333 k=4 repartition={'maxDT=5|0': {1, 2, 3}, 'maxDT=5|1': {4, 5}, 'maxDT=5|2': {6, 7}, 'maxDT=5|3': {8, 9}} \n",
      "stability=2 similarity=0.217778 k=3 repartition={'maxDT=3|0': {1, 2, 3}, 'maxDT=3|1': {8, 9}, 'maxDT=3|2': {4, 5, 6, 7}} \n",
      "stability=1 similarity=0.462222 k=2 repartition={'maxDT=2|0': {1, 2, 3}, 'maxDT=2|1': {4, 5, 6, 7, 8, 9}} selected!\n",
      "stability=1 similarity=0.162963 k=1 repartition={'maxDT=1|0': {1, 2, 3, 4, 5, 6, 7, 8, 9}} \n"
     ]
    }
   ],
   "source": [
    "# 6 MaxDTlength(BaseClusterings)\n",
    "maxDT = len(baseClusterings.columns)\n",
    "# 7 BiClust  {instance sets of FCPs built from MaxDT base clusters}\n",
    "# here BiClust is a list of sets of FCPs build from maxDT baseClusters\n",
    "biClust = []\n",
    "filteredFCP = list(filter(lambda x: len(x) == maxDT, FCPs))\n",
    "filteredFCP.sort(key = str)\n",
    "consVctrs = []\n",
    "for tempSet in filteredFCP:\n",
    "    isInSet = True\n",
    "    for col in tempSet:\n",
    "        isInSet = isInSet & membershipMatrix[col]\n",
    "        \n",
    "    biClust.append(set(baseClusterings.index[isInSet].tolist()))\n",
    "# 8 Assign a label to each set in BiClust to build the first consensus vector and store \n",
    "# it in a list of vectors ConsVctrs\n",
    "assignLabel(biClust, maxDT, consVctrs)\n",
    "# 9 Build the remaining consensuses\n",
    "# 10 for DT = (MaxDT1) to 1 do\n",
    "for dt in range(maxDT - 1, 0, -1): \n",
    "    filteredFCP = list(filter(lambda x: len(x) == dt, FCPs))\n",
    "    filteredFCP.sort(key = str)\n",
    "    for tempSet in filteredFCP:\n",
    "        isInSet = True\n",
    "        for col in tempSet:\n",
    "            isInSet = isInSet & membershipMatrix[col]\n",
    "        # 11 BiClust  BiClust  {instance sets of FCPs built from DT base clusters}\n",
    "        biClust.append(set(baseClusterings.index[isInSet].tolist()))\n",
    "    # 12 Call the consensus function (Algo. 10)\n",
    "    consensusFunction10(biClust)\n",
    "    # 13 Assign a label to each set in BiClust to build a consensus vector and add it to \n",
    "    #  ConsVctrs\n",
    "    assignLabel(biClust, dt, consVctrs)\n",
    "\n",
    "# 15 Remove similar consensuses\n",
    "# 16 ST  Vector of 1s of length MaxDT\n",
    "st = [1] * maxDT\n",
    "# 17 for i = MaxDT to 2 do\n",
    "# in python the index starts with 0 -> using maxDT - 1 to 1\n",
    "i = maxDT - 1\n",
    "while i > 0:\n",
    "    # 18 Vi  ith consensus in ConsVctrs\n",
    "    vi = consVctrs[i]\n",
    "    # 19 for j = (i1) to 1 do\n",
    "    # in python the index starts with 0 -> (i1) to 0\n",
    "    j = i - 1\n",
    "    while j >= 0:\n",
    "        # 20 Vj  jth consensus in ConsVctrs\n",
    "        vj = consVctrs[j]\n",
    "        if jaccard(vi, vj) == 1:\n",
    "            st[i] += 1\n",
    "            del st[j]\n",
    "            del consVctrs[j]\n",
    "            i -= 1\n",
    "            \n",
    "        j -= 1\n",
    "        \n",
    "    i -= 1\n",
    "# 27 Find the consensus the most similar to the ensemble\n",
    "# 28 L  length(ConsVctrs)\n",
    "L = len(consVctrs)\n",
    "# 29 TSim  Vector of 0s of lengthL\n",
    "tSim = [0] * L\n",
    "# 30 for i = 1 to L do\n",
    "for i in range(0, L):\n",
    "    # 31 Ci  ith consensus in ConsVctrs\n",
    "    ci = consVctrs[i]\n",
    "    # here tempArray will be ci converted from dict \n",
    "    #          {'maxDT=5|0': {1, 2, 3}, 'maxDT=5|1': {4, 5} ...\n",
    "    # to array [0, 0, 0, 1, 1,...]\n",
    "    tempArray = [-1] * len(baseClusterings.index)\n",
    "    for indx, key in enumerate(ci):\n",
    "        for ii in list(ci[key]):\n",
    "            tempArray[baseClusterings.index.tolist().index(ii)] = indx\n",
    "    # now tempArray is converted and equivalent to ci in the pseudo code\n",
    "    # 32 for j = 1 to MaxDT do\n",
    "    for j in range(maxDT):\n",
    "        # 33 Cj  jth clustering in BaseClusterings\n",
    "        cj = baseClusterings.iloc[:,j]\n",
    "        # 34 TSim[i]  TSim[i] + Jaccard(Ci,Cj)\n",
    "        tSim[i] += jaccard_score(tempArray, cj, average='macro')\n",
    "    # 36 Sim[i]  TSim[i] / MaxDT\n",
    "    tSim[i] /= maxDT\n",
    "\n",
    "for i in range(0, len(tSim)):\n",
    "    selectedConsensus = consVctrs[i]\n",
    "    # 38 RecommCons  which.max(TSim)\n",
    "    # we add 'selected' to the corresponding consensus\n",
    "    isSelected = 'selected!' if i == tSim.index(max(tSim)) else ''\n",
    "    clustAlgo_labels = pd.Series(name='consensus', index=baseClusterings.index, dtype='int')\n",
    "    for ii, key in enumerate(selectedConsensus):\n",
    "        for indx in selectedConsensus[key]:\n",
    "            clustAlgo_labels[indx] = ii\n",
    "            \n",
    "#     score = customClusteringScore(encoders, clustAlgo_labels, testY)\n",
    "#     print('stability=%i similarity=%f customScore=%f k=%i %s' % \\\n",
    "#           (st[i], tSim[i], score, clustAlgo_labels.nunique(), isSelected))\n",
    "    print('stability=%i similarity=%f k=%i repartition=%s %s' % \\\n",
    "          (st[i], tSim[i], clustAlgo_labels.nunique(), str(consVctrs[i]), isSelected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected permutation = ('maxDT=5|0', 'maxDT=5|2', 'maxDT=5|3', 'maxDT=5|1')\n",
      "selected permutation = ('maxDT=3|2', 'maxDT=3|0', 'maxDT=3|1')\n",
      "selected permutation = ('maxDT=2|0', 'maxDT=2|1')\n",
      "selected permutation = ('maxDT=1|0',)\n",
      "stability=1 similarity=0.331119 k=4 repartition={'maxDT=5|0': {1, 2, 3}, 'maxDT=5|1': {4, 5}, 'maxDT=5|2': {6, 7}, 'maxDT=5|3': {8, 9}} \n",
      "stability=2 similarity=0.457143 k=3 repartition={'maxDT=3|0': {1, 2, 3}, 'maxDT=3|1': {8, 9}, 'maxDT=3|2': {4, 5, 6, 7}} \n",
      "stability=1 similarity=0.476923 k=2 repartition={'maxDT=2|0': {1, 2, 3}, 'maxDT=2|1': {4, 5, 6, 7, 8, 9}} selected!\n",
      "stability=1 similarity=0.271209 k=1 repartition={'maxDT=1|0': {1, 2, 3, 4, 5, 6, 7, 8, 9}} \n"
     ]
    }
   ],
   "source": [
    "# trying to improve simirarity by rearrranging the partitions\n",
    "# for example = for clustering [0,0,0,1,1,2,2] trying also:\n",
    "# [1,1,1,0,0,2,2] , [2,2,2,1,1,0,0] , [0,0,0,2,2,1,1], etc...\n",
    "\n",
    "L = len(consVctrs)\n",
    "tSim = [0] * L\n",
    "for i in range(0, L):\n",
    "    ci = consVctrs[i]\n",
    "    tempSimilaritysVec = []\n",
    "    permutations = []\n",
    "    for perm in itertools.permutations(ci):\n",
    "        tempSimilarity = 0\n",
    "        tempArray = [-1] * len(baseClusterings.index)\n",
    "        indx = 0\n",
    "        for key in perm:\n",
    "            for ii in list(ci[key]):\n",
    "                tempArray[baseClusterings.index.tolist().index(ii)] = indx\n",
    "            indx += 1\n",
    "        for j in baseClusterings.columns:\n",
    "            cj = baseClusterings[j]\n",
    "            tempSimilarity += jaccard_score(cj, tempArray, average='micro')\n",
    "        tempSimilaritysVec.append(tempSimilarity/maxDT)\n",
    "        permutations.append(perm)\n",
    "    tSim[i] = max(tempSimilaritysVec)\n",
    "    print('selected permutation =', permutations[tempSimilaritysVec.index(tSim[i])])\n",
    "\n",
    "for i in range(0, len(tSim)):\n",
    "    selectedConsensus = consVctrs[i]\n",
    "    isSelected = 'selected!' if i == tSim.index(max(tSim)) else ''\n",
    "    clustAlgo_labels = pd.Series(name='consensus', index=baseClusterings.index, dtype='int')\n",
    "    for ii, key in enumerate(selectedConsensus):\n",
    "        for indx in selectedConsensus[key]:\n",
    "            clustAlgo_labels[indx] = ii\n",
    "            \n",
    "#     score = customClusteringScore(encoders, clustAlgo_labels, testY)\n",
    "#     print('stability=%i similarity=%f customScore=%f k=%i %s' % \\\n",
    "#           (st[i], tSim[i], score, clustAlgo_labels.nunique(), isSelected))\n",
    "    print('stability=%i similarity=%f k=%i repartition=%s %s' % \\\n",
    "          (st[i], tSim[i], clustAlgo_labels.nunique(), str(consVctrs[i]), isSelected))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
